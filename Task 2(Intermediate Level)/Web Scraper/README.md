# ShadowFox Web Scraper
![alt text](image.png)

![ShadowFox](https://img.shields.io/badge/ShadowFox-Web%20Scraper-blue)
![Python](https://img.shields.io/badge/Python-3.8%2B-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ¦Š About

**ShadowFox Web Scraper** is a professional-grade web scraping toolkit developed by the ShadowFox Development Team. This project demonstrates advanced web scraping techniques with comprehensive error handling, multi-threading, and multiple export formats.

**Website:** [https://www.shadowfox.org.in/](https://www.shadowfox.org.in/)

**Tagline:** LEARN â€¢ CREATE â€¢ LEAD

---

## âœ¨ Features

### Basic Scraper (`app.py`)
- âœ… Beautiful Soup integration for HTML parsing
- âœ… Comprehensive error handling (HTTP, Connection, Timeout errors)
- âœ… Multiple export formats (JSON, CSV, TXT)
- âœ… Retry logic with exponential backoff
- âœ… Session management
- âœ… Logging system (console + file)
- âœ… Statistics tracking
- âœ… ShadowFox branding

### Advanced Scraper (`advanced_scraper.py`)
- ğŸš€ Multi-threaded crawling
- ğŸš€ Deep link discovery
- ğŸš€ Pattern matching
- ğŸš€ Configurable crawl depth
- ğŸš€ URL normalization
- ğŸš€ Progress tracking with colored output
- ğŸš€ Comprehensive data extraction
- ğŸš€ Respectful crawling with rate limiting

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Install Dependencies

```bash
pip install requests beautifulsoup4
```

---

## ğŸš€ Usage

### Basic Scraper

```python
from app import WebScraperPro, scrape_shadowfox_website, demo_scraper

# Run demonstration scraper
demo_scraper()

# Or scrape ShadowFox website
scrape_shadowfox_website()
```

**Run from command line:**
```bash
python app.py
```

### Advanced Scraper

```python
from advanced_scraper import AdvancedWebScraper

# Create scraper instance
scraper = AdvancedWebScraper(
    target_url="http://example.com/",
    max_depth=2,
    max_threads=4
)

# Start crawling
scraper.start_crawling()
```

**Run from command line:**
```bash
python advanced_scraper.py
```

---

## ğŸ“Š Output Files

All scraped data is saved in the `scraped_data/` directory:

### Basic Scraper Output
- `shadowfox_data_TIMESTAMP.json` - Complete scraped data in JSON format
- `shadowfox_report_TIMESTAMP.txt` - Human-readable report
- `shadowfox_links_TIMESTAMP.csv` - Extracted links in CSV format
- `scraper_TIMESTAMP.log` - Detailed log file

### Advanced Scraper Output
- `crawl_results_TIMESTAMP.json` - Complete crawl results with statistics
- `discovered_urls_TIMESTAMP.txt` - List of all discovered URLs
- `crawler_TIMESTAMP.log` - Detailed crawl log

---

## ğŸ¨ Features Breakdown

### Data Extraction

The scrapers extract:
- **Metadata**: Title, description, keywords, Open Graph tags
- **Content**: Headings (H1-H6), paragraphs, lists
- **Links**: Internal and external links with text
- **Images**: Image sources, alt text, titles
- **Forms**: Form actions and methods
- **Scripts**: External script sources

### Error Handling

Comprehensive error handling for:
- HTTP errors (404, 403, 500, etc.)
- Connection errors
- Timeout errors
- Request exceptions
- Unexpected errors

### Rate Limiting

- Configurable delay between requests
- Respectful crawling to avoid overwhelming servers
- Session management for efficient requests

---

## âš™ï¸ Configuration

### Basic Scraper Configuration

```python
class ScraperConfig:
    DEFAULT_TIMEOUT = 15
    DEFAULT_DELAY = 2
    MAX_RETRIES = 3
    OUTPUT_DIR = Path('scraped_data')
```

### Advanced Scraper Configuration

```python
class AdvancedScraperConfig:
    DEFAULT_TIMEOUT = 15
    DEFAULT_DELAY = 1
    MAX_RETRIES = 3
    MAX_DEPTH = 3
    MAX_THREADS = 4
    OUTPUT_DIR = Path('scraped_data')
```

---

## ğŸ“ˆ Statistics

The scrapers track:
- Total requests made
- Successful requests
- Failed requests
- Data points extracted
- URLs discovered
- Crawling duration
- Average speed (pages/second)

---

## ğŸ›¡ï¸ Best Practices

1. **Respect robots.txt**: Always check the website's robots.txt file
2. **Rate limiting**: Use appropriate delays between requests
3. **User-Agent**: Use a descriptive User-Agent header
4. **Error handling**: Handle errors gracefully
5. **Legal compliance**: Ensure you have permission to scrape the website
6. **Data privacy**: Respect user privacy and data protection laws

---

## ğŸ“ Example Output

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                            â•‘
â•‘                          SHADOWFOX WEB SCRAPER                             â•‘
â•‘                           Advanced Edition v2.0                            â•‘
â•‘                                                                            â•‘
â•‘                        LEARN â€¢ CREATE â€¢ LEAD                               â•‘
â•‘                    https://www.shadowfox.org.in/                           â•‘
â•‘                                                                            â•‘
â•‘         Unleash the power of Open-Source Intelligence                      â•‘
â•‘                  ~By: ShadowFox Development Team                           â•‘
â•‘                                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TARGET: http://quotes.toscrape.com/

[!] Initializing Crawler...
[!] Max Depth: 2
[!] Max Threads: 4
[âœ“] Preparing Crawler (Utilizing 4 threads)

[!]Crawling: http://quotes.toscrape.com/
  â†’ [âœ“] Matched: ['quotes']

Crawling finished.

[âœ“] Results exported to: scraped_data\crawl_results_20251107_225647.json
[âœ“] URLs exported to: scraped_data\discovered_urls_20251107_225647.txt

================================================================================
CRAWLING STATISTICS
================================================================================
Target URL:           http://quotes.toscrape.com/
URLs Crawled:         15
URLs Discovered:      47
Pages Extracted:      15
Errors Encountered:   0
Duration:             12.34 seconds
Average Speed:        1.22 pages/second
================================================================================
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ‘¥ Authors

**ShadowFox Development Team**

- Website: [https://www.shadowfox.org.in/](https://www.shadowfox.org.in/)
- Tagline: LEARN â€¢ CREATE â€¢ LEAD

---

## âš ï¸ Disclaimer

This tool is for educational purposes only. Always ensure you have permission to scrape a website and comply with its terms of service and robots.txt file. The developers are not responsible for any misuse of this tool.

---

## ğŸ™ Acknowledgments

- Beautiful Soup library for HTML parsing
- Requests library for HTTP requests
- ShadowFox community for support and feedback

---

**Made with â¤ï¸ by ShadowFox Development Team**
